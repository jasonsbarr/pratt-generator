import { lexer, rule } from "@jasonsbarr/lexer";
import { createParser } from "./src/parser.js";

const keywords = ["true", "false", "null", "lam", "if", "else", "then"];

const rules = keywords
  .map((k) => rule("Keyword", k.toUpperCase(), k))
  .concat([
    rule("WS", "WS", String.raw`\s+`),
    rule("Number", "NUMBER", String.raw`\d+`),
    rule("String", "STRING", String.raw`"(?:\\.|[^\\"])*"?`),
    rule("Symbol", "ARROW", String.raw`->`),
    rule("Symbol", "INC", String.raw`\+\+`),
    rule("Symbol", "PLUS", String.raw`\+`),
    rule("Symbol", "MINUS", String.raw`-`),
    rule("Symbol", "EXP", String.raw`\*\*`),
    rule("Symbol", "MUL", String.raw`\*`),
    rule("Symbol", "DIV", String.raw`/`),
    rule("Punc", "LPAREN", String.raw`\(`),
    rule("Punc", "RPAREN", String.raw`\)`),
    rule("Punc", "COMMA", String.raw`,`),
    rule("Symbol", "ASSIGN", String.raw`=`),
    rule("Symbol", "IDENT", String.raw`[a-zA-Z_][\w]*`),
  ]);

const lex = lexer(rules);

const filterWs = (tokens) => [...tokens].filter((t) => t.type !== "WS");

export const tokenize = (input) =>
  filterWs(lex.compile().input(input).tokenize());

const operators = [
  {
    id: "NumberLiteral",
    nToken: "NUMBER",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "StringLiteral",
    nToken: "STRING",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "TrueLiteral",
    nToken: "TRUE",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "FalseLiteral",
    nToken: "FALSE",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "NullLiteral",
    nToken: "NULL",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "Identifier",
    nToken: "IDENT",
    lToken: null,
    oToken: null,
    prec: 0,
    assoc: "NONE",
    affix: "NONE",
    arity: "NONE",
  },
  {
    id: "Plus",
    nToken: null,
    lToken: "PLUS",
    oToken: null,
    prec: 30,
    assoc: "LEFT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "Minus",
    nToken: null,
    lToken: "MINUS",
    oToken: null,
    prec: 30,
    assoc: "LEFT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "Mul",
    nToken: null,
    lToken: "MUL",
    oToken: null,
    prec: 40,
    assoc: "LEFT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "Div",
    nToken: null,
    lToken: "DIV",
    oToken: null,
    prec: 40,
    assoc: "LEFT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "Exp",
    nToken: null,
    lToken: "EXP",
    oToken: null,
    prec: 45,
    assoc: "RIGHT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "UPlus",
    nToken: "PLUS",
    lToken: null,
    oToken: null,
    prec: 50,
    assoc: "RIGHT",
    affix: "PREFIX",
    arity: "UNARY",
  },
  {
    id: "UMinus",
    nToken: "MINUS",
    lToken: null,
    oToken: null,
    prec: 50,
    assoc: "RIGHT",
    affix: "PREFIX",
    arity: "UNARY",
  },
  {
    id: "Parentheses",
    nToken: "LPAREN",
    lToken: null,
    oToken: "RPAREN",
    prec: 100,
    assoc: "NONE",
    affix: "MATCHFIX",
    arity: "UNARY",
  },
  {
    id: "IfElse",
    nToken: null,
    lToken: "IF",
    oToken: "ELSE",
    prec: 5,
    assoc: "LEFT",
    affix: "INFIX",
    arity: "TERNARY",
  },
  {
    id: "IfThenElse",
    nToken: "IF",
    lToken: "THEN",
    oToken: "ELSE",
    prec: 5,
    assoc: "LEFT",
    affix: "MIXFIX",
    arity: "TERNARY",
  },
  {
    id: "Assignment",
    nToken: null,
    lToken: "ASSIGN",
    oToken: null,
    prec: 3,
    assoc: "RIGHT",
    affix: "INFIX",
    arity: "BINARY",
  },
  {
    id: "Increment",
    nToken: null,
    lToken: "INC",
    oToken: null,
    prec: 60,
    assoc: "LEFT",
    affix: "POSTFIX",
    arity: "UNARY",
  },
  {
    id: "Lambda",
    nToken: "LAM",
    lToken: "ARROW",
    oToken: null,
    prec: 5,
    assoc: "RIGHT",
    affix: "MIXFIX",
    arity: "BINARY",
  },
];

const parser = createParser(operators);

export const parse = (input) => parser(tokenize(input));
